{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc99325",
        "language": "markdown"
      },
      "source": [
        "# Gradio Chatbot UI for Multi-Agent System\n",
        "Interact with your AI agents using a conversational interface powered by Gradio and the logic in `mas.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5bd0641f",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import gradio as gr\n",
        "from src.mas import Orchestrator\n",
        "\n",
        "# Initialize the orchestrator\n",
        "orchestrator = Orchestrator()\n",
        "\n",
        "# Global chat history\n",
        "chat_history = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d022945",
        "language": "markdown"
      },
      "source": [
        "## Define the Chatbot Logic\n",
        "This function will be called on each user message. It sends the message to the orchestrator and returns the updated chat history, clears the input, and optionally a filename to plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7b16e030",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def chat_with_agents(user_message, history):\n",
        "    global chat_history\n",
        "    file_name = None\n",
        "    if not user_message.strip():\n",
        "        return history, \"\", None\n",
        "    # Append user message to history for display\n",
        "    history = history or []\n",
        "    history.append((\"You\", user_message))\n",
        "    # Run orchestrator and get agent responses\n",
        "    try:\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        results, file_name = loop.run_until_complete(orchestrator.run(user_message))\n",
        "        # Append each response from the orchestrator\n",
        "        for result in results:\n",
        "            role = result.get(\"role\", \"\")\n",
        "            name = result.get(\"name\", \"\")\n",
        "            content = result.get(\"content\", \"\")\n",
        "            history.append((name or role, content))\n",
        "            # file_name is obtained from orchestrator return value\n",
        "    except Exception as e:\n",
        "        history.append((\"system\", f\"Error: {e}\"))\n",
        "    return history, \"\", file_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ef093ce",
        "language": "markdown"
      },
      "source": [
        "## Build the Gradio UI\n",
        "This UI includes a chatbot window, a textbox for user input, and an image component to display any returned image file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c754a27",
      "metadata": {
        "id": "c13717ec",
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for debugger to attach...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3698792/2709583654.py:12: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Conversation\", height=800)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# AuthorRole.USER: 'Help me to interpret the AI model's results'\n",
            "# AuthorRole.USER: 'Nice, could you recommend some charts?'\n",
            "# AuthorRole.USER: 'Could you plot the Summary Plot?'\n",
            "File downloaded: assistant-7xqRpPGoSs5wK2qWudvKK5.png\n",
            "# AuthorRole.USER: 'Now, could you plot the feature importance bar chart?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Polling timed out for run id: `run_UfHlRoQovDjAShsxIzGGa3VV` and thread id: `thread_RgWGcmH1vYoCkUsXbcsvSnjD` after waiting 0:01:00.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: Polling timed out for run id: `run_UfHlRoQovDjAShsxIzGGa3VV` and thread id: `thread_RgWGcmH1vYoCkUsXbcsvSnjD` after waiting 0:01:00.\n",
            "# AuthorRole.USER: 'Now, could you plot the feature importance bar chart?'\n",
            "An error occurred: Run failed with status: `RunStatus.FAILED` for agent `AnalystAgent` and thread `thread_IFWJ28CcssB4bm1UhrqHVNPA` with error: Sorry, something went wrong.\n"
          ]
        }
      ],
      "source": [
        "import debugpy\n",
        "\n",
        "# Allow other clients to attach to debugpy\n",
        "debugpy.listen((\"0.0.0.0\", 5678))\n",
        "print(\"Waiting for debugger to attach...\")\n",
        "\n",
        "# Pause execution until the debugger is attached\n",
        "debugpy.wait_for_client()\n",
        "\n",
        "with gr.Blocks(title=\"Reasoning over ML - Chatbot\") as demo:\n",
        "    gr.Markdown(\"\"\"# Multi-Agent Chatbot\\nChat with your AI agents below.\"\"\")\n",
        "    chatbot = gr.Chatbot(label=\"Conversation\", height=700)\n",
        "    textbox = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message and press Enter\", show_label=True, lines=1)\n",
        "    image = gr.Image(type=\"filepath\", label=\"Generated Image\")\n",
        "\n",
        "    def clear_history():\n",
        "        global chat_history\n",
        "        chat_history = []\n",
        "        return [], None\n",
        "\n",
        "    textbox.submit(chat_with_agents, [textbox, chatbot], [chatbot, textbox, image])\n",
        "    gr.Button(\"Clear\").click(clear_history, outputs=[chatbot, image])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
