{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# Gradio Chatbot UI for Multi-Agent System\n",
        "Interact with your AI agents using a conversational interface powered by Gradio and the logic in `mas.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import gradio as gr\n",
        "from src.mas import Orchestrator\n",
        "\n",
        "# Initialize the orchestrator\n",
        "orchestrator = Orchestrator()\n",
        "\n",
        "# Global chat history\n",
        "chat_history = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Define the Chatbot Logic\n",
        "This function will be called on each user message. It sends the message to the orchestrator and returns the updated chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def chat_with_agents(user_message, history):\n",
        "    global chat_history\n",
        "    if not user_message.strip():\n",
        "        return history, \"\"\n",
        "    # Append user message to history for display\n",
        "    history = history or []\n",
        "    history.append((\"You\", user_message))\n",
        "    # Run orchestrator and get agent responses\n",
        "    try:\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        results = loop.run_until_complete(orchestrator.run(user_message))\n",
        "        # Append each response from the orchestrator\n",
        "        for result in results:\n",
        "            role = result.get(\"role\", \"\")\n",
        "            name = result.get(\"name\", \"\")\n",
        "            content = result.get(\"content\", \"\")\n",
        "            history.append((name or role, content))\n",
        "    except Exception as e:\n",
        "        history.append((\"system\", f\"Error: {e}\"))\n",
        "    return history, \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Build the Gradio UI\n",
        "This UI includes a chatbot window and a textbox for user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for debugger to attach...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3691716/2020754318.py:12: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Conversation\", height=400)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# AuthorRole.USER: 'Help me to interpret the AI model's results'\n",
            "# AuthorRole.USER: 'Nice, could you recommend some charts?'\n",
            "File downloaded: assistant-4oi8NS3fiFWqE7wHvwQaUn.png\n"
          ]
        }
      ],
      "source": [
        "import debugpy\n",
        "\n",
        "# Allow other clients to attach to debugpy\n",
        "debugpy.listen((\"0.0.0.0\", 5678))\n",
        "print(\"Waiting for debugger to attach...\")\n",
        "\n",
        "# Pause execution until the debugger is attached\n",
        "debugpy.wait_for_client()\n",
        "\n",
        "with gr.Blocks(title=\"Reasoning over ML - Chatbot\") as demo:\n",
        "    gr.Markdown(\"\"\"# Multi-Agent Chatbot\\nChat with your AI agents below.\"\"\")\n",
        "    chatbot = gr.Chatbot(label=\"Conversation\", height=400)\n",
        "    textbox = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message and press Enter\", show_label=True, lines=1)\n",
        "\n",
        "    def clear_history():\n",
        "        global chat_history\n",
        "        chat_history = []\n",
        "        return []\n",
        "\n",
        "    textbox.submit(chat_with_agents, [textbox, chatbot], [chatbot, textbox])\n",
        "    gr.Button(\"Clear\").click(clear_history, outputs=chatbot)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
